{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee102ddb",
   "metadata": {},
   "source": [
    "#### Home Work - 2                                                                                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b0cdd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "# Load important libraries\n",
    "import nltk\n",
    "from nltk.book import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from urllib import request\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba317b76",
   "metadata": {},
   "source": [
    "#### 1.\tIn Python, create a method for scoring the vocabulary size of a text, and normalize the score from 0 to 1. It does not matter what method you use for normalization as long as you explain it in a short paragraph. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fac4ed2",
   "metadata": {},
   "source": [
    "Normaliztion can be done using Min Max Scaler from the sklearn module or it can also be done using the simple normalization formula. The formula is dividing the difference of each text's vocabulary size to the min vocabular size on the list by the difference between the max and the min vocabulary size ((x - min(x) / (max(x) - min(x)))). This ensures that the score is normalized between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a397e757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_size(*arg):\n",
    "    vocab_size = np.array([])\n",
    "    vocab_size_norm = np.array([])\n",
    "    vocab_size_norm_sklearn = np.array([])\n",
    "    \n",
    "    #### Getting the Vocab Size after converting all letters to lower case and removing the words which are not alphabets\n",
    "    for text in arg:\n",
    "        vocab_size = np.append(vocab_size,len(set(word.lower() for word in text if word.isalpha())))\n",
    "        \n",
    "    #### Normalizing the vocabulary size using formula (x - min(x) / (max(x) - min(x)))\n",
    "    for vsize in vocab_size:\n",
    "        vocab_size_norm = np.append(vocab_size_norm,(vsize - vocab_size.min()) /\n",
    "                                                    (vocab_size.max() - vocab_size.min()))\n",
    "    \n",
    "    #### Normalizing the vocabulary size using sklearn Min Max Scaler \n",
    "    vocab_size_norm_sklearn = minmax_scale(vocab_size, feature_range=(0,1), axis=0)\n",
    "    \n",
    "    return(vocab_size,vocab_size_norm,vocab_size_norm_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a6f04f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_dtls = vocab_size(text1,text2,text3,text4,text5,text6,text7,text8,text9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46e42506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vocabulary Size</th>\n",
       "      <th>Normaliztion using the simple formula</th>\n",
       "      <th>Normalization using the sklearn module</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16948.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6283.0</td>\n",
       "      <td>0.340731</td>\n",
       "      <td>0.340731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2615.0</td>\n",
       "      <td>0.113989</td>\n",
       "      <td>0.113989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9110.0</td>\n",
       "      <td>0.515485</td>\n",
       "      <td>0.515485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4671.0</td>\n",
       "      <td>0.241083</td>\n",
       "      <td>0.241083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1799.0</td>\n",
       "      <td>0.063547</td>\n",
       "      <td>0.063547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9109.0</td>\n",
       "      <td>0.515423</td>\n",
       "      <td>0.515423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>771.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6300.0</td>\n",
       "      <td>0.341782</td>\n",
       "      <td>0.341782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Vocabulary Size  Normaliztion using the simple formula  \\\n",
       "0          16948.0                               1.000000   \n",
       "1           6283.0                               0.340731   \n",
       "2           2615.0                               0.113989   \n",
       "3           9110.0                               0.515485   \n",
       "4           4671.0                               0.241083   \n",
       "5           1799.0                               0.063547   \n",
       "6           9109.0                               0.515423   \n",
       "7            771.0                               0.000000   \n",
       "8           6300.0                               0.341782   \n",
       "\n",
       "   Normalization using the sklearn module  \n",
       "0                                1.000000  \n",
       "1                                0.340731  \n",
       "2                                0.113989  \n",
       "3                                0.515485  \n",
       "4                                0.241083  \n",
       "5                                0.063547  \n",
       "6                                0.515423  \n",
       "7                                0.000000  \n",
       "8                                0.341782  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dataframe\n",
    "vocab_size_summary = pd.DataFrame({'Vocabulary Size': vocab_size_dtls[0], \n",
    "                                   'Normaliztion using the simple formula': vocab_size_dtls[1], \n",
    "                                   'Normalization using the sklearn module': vocab_size_dtls[2]})\n",
    "\n",
    "vocab_size_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce8fb84",
   "metadata": {},
   "source": [
    "Normalized vocabulary scores for texts from NLTK library are same either calculated with simple formula or with Min Max Scaler from the sklearn module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6118f84",
   "metadata": {},
   "source": [
    "#### 2.\tAfter consulting section 3.2 in chapter 1 of Bird-Klein, create a method for scoring the long-word vocabulary size of a text, and likewise normalize (and explain) the scoring as in step 1 above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f832287",
   "metadata": {},
   "source": [
    "Below method calculates the vocabulary size in a particular corpus. While calculating the vocabulary size it considereds only the words with more than 7 letters and words repeat more than 7 times in the corpus. This eliminates frequent short words (e.g., the) and infrequent long words in the text. Then calculates the normalized score using simple formula and Min Max Scaler from the sklearn module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae3c1788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_word_score(*arg, word_len=7, word_freq=7):\n",
    "    long_word_score = np.array([])\n",
    "    long_word_score_norm = np.array([])\n",
    "    long_word_score_norm_sklearn = np.array([])\n",
    "    \n",
    "    #### Getting the Vocab Size of long words with word lenth of 7 characters and word repeats atleast 7 times\n",
    "    for text in arg:\n",
    "        fdist = FreqDist(text)\n",
    "        size = len(sorted(word for word in set(text) if len(word) > word_len and fdist[word] > word_freq))\n",
    "        long_word_score = np.append(long_word_score,size)\n",
    "        \n",
    "    #### Normalizing the vocabulary size for long words using formula (x - min(x) / (max(x) - min(x)))\n",
    "    for vsize in long_word_score:\n",
    "        long_word_score_norm = np.append(long_word_score_norm,(vsize - long_word_score.min()) /\n",
    "                                                    (long_word_score.max() - long_word_score.min()))\n",
    "    \n",
    "    #### Normalizing the vocabulary size for long words using sklearn Min Max Scaler \n",
    "    long_word_score_norm_sklearn = minmax_scale(long_word_score, feature_range=(0,1), axis=0)\n",
    "    \n",
    "    return(long_word_score,long_word_score_norm,long_word_score_norm_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac06fd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_score = long_word_score(text1,text2,text3,text4,text5,text6,text7,text8,text9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88a92c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Long Word Vocabulary Size</th>\n",
       "      <th>Normaliztion using the simple formula</th>\n",
       "      <th>Normalization using the sklearn module</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>555.0</td>\n",
       "      <td>0.827795</td>\n",
       "      <td>0.827795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>399.0</td>\n",
       "      <td>0.592145</td>\n",
       "      <td>0.592145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.061934</td>\n",
       "      <td>0.061934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>669.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.018127</td>\n",
       "      <td>0.018127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.0</td>\n",
       "      <td>0.007553</td>\n",
       "      <td>0.007553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>369.0</td>\n",
       "      <td>0.546828</td>\n",
       "      <td>0.546828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.141994</td>\n",
       "      <td>0.141994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Long Word Vocabulary Size  Normaliztion using the simple formula  \\\n",
       "0                      555.0                               0.827795   \n",
       "1                      399.0                               0.592145   \n",
       "2                       48.0                               0.061934   \n",
       "3                      669.0                               1.000000   \n",
       "4                       19.0                               0.018127   \n",
       "5                       12.0                               0.007553   \n",
       "6                      369.0                               0.546828   \n",
       "7                        7.0                               0.000000   \n",
       "8                      101.0                               0.141994   \n",
       "\n",
       "   Normalization using the sklearn module  \n",
       "0                                0.827795  \n",
       "1                                0.592145  \n",
       "2                                0.061934  \n",
       "3                                1.000000  \n",
       "4                                0.018127  \n",
       "5                                0.007553  \n",
       "6                                0.546828  \n",
       "7                                0.000000  \n",
       "8                                0.141994  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dataframe\n",
    "long_word_vocab_size_summary = pd.DataFrame({'Long Word Vocabulary Size': word_score[0], \n",
    "                                   'Normaliztion using the simple formula': word_score[1], \n",
    "                                   'Normalization using the sklearn module': word_score[2]})\n",
    "\n",
    "long_word_vocab_size_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101b9989",
   "metadata": {},
   "source": [
    "Normalized long word vocabulary scores with word length and word frequency of 7 for all books from NLTK library are same either calculated with simple formula or with Min Max Scaler from the sklearn module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e358dd7",
   "metadata": {},
   "source": [
    "#### 3.\tNow create a “text difficulty score” by combining the lexical diversity score from homework 1, and your normalized score of vocabulary size and long-word vocabulary size, in equal weighting. Explain what you see when this score is applied to same graded texts you used in homework1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9607c486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns text/tokens\n",
    "def getTextFromUrl(url, text=False):\n",
    "    # make a request\n",
    "    response = request.urlopen(url)\n",
    "\n",
    "    # extract the raw response\n",
    "    raw = response.read().decode('utf8')\n",
    "\n",
    "    # tokenize the words\n",
    "    tokens = nltk.word_tokenize(raw)\n",
    "\n",
    "    if text:\n",
    "        textObj = nltk.Text(tokens)\n",
    "        return textObj\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# get the book title\n",
    "def getBookTitle(url):\n",
    "    # make a request\n",
    "    response = request.urlopen(url)\n",
    "\n",
    "    # extract the raw response\n",
    "    raw = response.read().decode('utf8')\n",
    "\n",
    "    match = re.search(r'Title: ([\\w\\']+) (.+)', raw)\n",
    "    if match:\n",
    "        title = match.group()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    return title.strip('\\r')\n",
    "\n",
    "# get the lexical diversity \n",
    "def lexical_diversity(*arg):\n",
    "    lexical_diversity = np.array([])\n",
    "    \n",
    "    for text in arg:\n",
    "        lexical_diversity = np.append(lexical_diversity,len(set(text)) / len(text))\n",
    "    \n",
    "    return (lexical_diversity)\n",
    "\n",
    "# get text difficulty score\n",
    "def text_difficulty_score(*arg):\n",
    "    text_difficulty_score = np.array([])\n",
    "    \n",
    "    print(\"Lexical Diversity:\", lexical_diversity(arg[0], arg[1], arg[2]))\n",
    "    print(\"Vocabulary Score:\", vocab_size(arg[0], arg[1], arg[2])[2])\n",
    "    print(\"Long word Vocabulary score:\", long_word_score(arg[0], arg[1], arg[2])[2])\n",
    "    \n",
    "    text_difficulty_score = 1/3*(lexical_diversity(arg[0], arg[1], arg[2]) + \n",
    "                                 vocab_size(arg[0], arg[1], arg[2])[2] + \n",
    "                                 long_word_score(arg[0], arg[1], arg[2])[2])\n",
    "                                            \n",
    "    return text_difficulty_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a84a04dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url for third grade\n",
    "url1 = \"https://www.gutenberg.org/cache/epub/14766/pg14766.txt\"\n",
    "\n",
    "# url for fourth grade\n",
    "url2 = \"https://www.gutenberg.org/cache/epub/14880/pg14880.txt\"\n",
    "\n",
    "# url for fifth grade\n",
    "url3 = \"https://www.gutenberg.org/cache/epub/15040/pg15040.txt\"\n",
    "\n",
    "# create a list with URL's\n",
    "urls = [url1, url2, url3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8e37367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical Diversity: [0.12404917 0.12347441 0.11286183]\n",
      "Vocabulary Score: [0.         0.55148293 1.        ]\n",
      "Long word Vocabulary score: [0.         0.36111111 1.        ]\n"
     ]
    }
   ],
   "source": [
    "# get the text for each book\n",
    "texts = [getTextFromUrl(url, text=True) for url in urls]\n",
    "\n",
    "# get book title \n",
    "titles = [getBookTitle(url) for url in urls]\n",
    "\n",
    "# Calculate text difficulty score\n",
    "text_difficulty_score = text_difficulty_score(texts[0],texts[1],texts[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a160671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text difficulty score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Title: McGuffey's Third Eclectic Reader</td>\n",
       "      <td>0.041350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Title: McGuffey's Fourth Eclectic Reader</td>\n",
       "      <td>0.345356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Title: McGuffey's Fifth Eclectic Reader</td>\n",
       "      <td>0.704287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Title  Text difficulty score\n",
       "0   Title: McGuffey's Third Eclectic Reader               0.041350\n",
       "1  Title: McGuffey's Fourth Eclectic Reader               0.345356\n",
       "2   Title: McGuffey's Fifth Eclectic Reader               0.704287"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dataframe with title of the book and difficulty score\n",
    "summary = pd.DataFrame({'Title': titles, 'Text difficulty score': text_difficulty_score})\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6cf933",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "When we are comparing text difficulty among different texts with lexical diversity score alone from homework1,  third grade book (lexical diversity score 0.124049) is more difficulty than fourth (lexical diversity score 0.123474) and fifth grade (lexical diversity score 0.112862) books where lexical diversity is dependent on the total number of tokens. New measurement which combines the lexical score with the normalized score of the vocabulary size and the long-word vocabulary size, I believe that this difficulty score is more useful than the one in Homework 1 because it takes into account longer words, which tend to be more difficult to read, and the normalized size of the vocabulary, in addition to the lexical diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7703693e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
